{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_species = 11255 # Nombre de toutes les classes uniques dans les données PO et PA.\n",
    "num_classes = 5\n",
    "seed = 42\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset  # Type de sous-ensemble (ex: train, val, test)\n",
    "        self.transform = transform  # Transformation à appliquer aux échantillons\n",
    "        self.data_dir = data_dir  # Répertoire contenant les fichiers de données\n",
    "        self.metadata = metadata  # Données de métadonnées\n",
    "        # Supprimer les lignes sans speciesId et réinitialiser les index\n",
    "        self.metadata = self.metadata.dropna(subset=[\"speciesId\"]).reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)  # Convertir speciesId en entier\n",
    "        \n",
    "        # Colonnes des labels de présence\n",
    "        self.label_columns = ['absence', 'presence_a_2_digit', \n",
    "                              'presence_a_3_digit', 'presence_a_4_digit', 'presence_seule']\n",
    "        \n",
    "        # Créer un dictionnaire des labels par surveyId\n",
    "        self.label_dict = self.metadata.groupby('surveyId', group_keys=False).apply(\n",
    "            lambda x: x.set_index('speciesId')[self.label_columns].to_dict(orient='index')\n",
    "        ).to_dict()\n",
    "        \n",
    "        # Supprimer les doublons de surveyId et réinitialiser les index\n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Retourne le nombre de surveyId uniques\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC24-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        labels = self.label_dict.get(survey_id, {})\n",
    "        label = torch.zeros((num_species, len(self.label_columns)))  \n",
    "        label[:,0] = 1\n",
    "        # Remplir le tenseur de labels avec les données de présence\n",
    "        for species_id, presence_data in labels.items():\n",
    "            if species_id < num_species:  # S'assurer que species_id est dans la plage valide\n",
    "                label[species_id] = torch.tensor(list(presence_data.values()), dtype=torch.float32)\n",
    "\n",
    "        # S'assurer que l'échantillon est au bon format pour la transformation\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            # Changer la forme du tenseur de (C, H, W) à (H, W, C)\n",
    "            sample = sample.permute(1, 2, 0)  \n",
    "            sample = sample.numpy()  \n",
    "\n",
    "        # Appliquer la transformation si elle est définie\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # Retourner l'échantillon, les labels et le surveyId\n",
    "        return sample, label, survey_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#Load Training metadata\n",
    "train_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-train-landsat-time-series\"\n",
    "train_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/metadata_with_classes.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, subset=\"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-train-landsat-time-series\"\n",
    "test_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/metadata_with_classes_test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TrainDataset(test_data_path, test_metadata, subset=\"train\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, num_species):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        self.norm_input = nn.LayerNorm([6, 4, 21])\n",
    "        self.resnet18 = models.resnet18(weights=None)\n",
    "        self.resnet18.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet18.maxpool = nn.Identity()\n",
    "        self.ln = nn.LayerNorm(1000)\n",
    "        self.fc0 = nn.Linear(1000, 2056)\n",
    "        self.fc1 = nn.Linear(2056, num_species*num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_input(x)\n",
    "        x = self.resnet18(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, num_species, num_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dakbarin/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ModifiedResNet18(num_species).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a085b68e069f4e51ac3ab2b01cc6abb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28, Training Loss: 8690.983835371713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28, Validation Loss: 5566.052783203125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/28, Training Loss: 3751.296443394252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/28, Validation Loss: 236.05105743408203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3706978426430480bb2f41036e5e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 28\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, targets, _ in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        \n",
    "        # Convertir les targets en indices de classe\n",
    "        target_indices = torch.argmax(targets, dim=2)\n",
    "        \n",
    "        # Calculer la perte pour chaque espèce individuellement\n",
    "        loss = 0.0\n",
    "        for i in range(num_species):\n",
    "            loss += criterion(out[:, i, :], target_indices[:, i])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, targets, _ in tqdm(test_loader, desc=\"Validation\", leave=False):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            out = model(data)\n",
    "            \n",
    "            target_indices = torch.argmax(targets, dim=2)\n",
    "\n",
    "            # Calculer la perte pour chaque espèce individuellement\n",
    "            loss = 0.0\n",
    "            for i in range(num_species):\n",
    "                loss += criterion(out[:, i, :], target_indices[:, i])\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(test_loader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"resnet18_with_bioclimatic_cubes2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_species = 11255\n",
    "batch_size = 64\n",
    "device = 'cuda'\n",
    "def remove_module_prefix(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "# Charger l'état du modèle entraîné\n",
    "model = ModifiedResNet18(num_species).to(device)\n",
    "state_dict = torch.load(\"/home/dakbarin/data/models/resnet18_with_bioclimatic_cubes_epoch_16.pth\")\n",
    "state_dict = remove_module_prefix(state_dict)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=[\"speciesId\"]).reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC24-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])\n",
    "        label = torch.zeros(num_species)\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1\n",
    "\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)\n",
    "            sample = sample.numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label, survey_id\n",
    "\n",
    "# Définir la transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Charger les métadonnées et les données de test\n",
    "test_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-train-landsat-time-series\"\n",
    "test_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24_PA_metadata_train.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path, delimiter=';').iloc[500000:600001]\n",
    "test_dataset = TestDataset(test_data_path, test_metadata, subset=\"train\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.019883471765074007\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, num_species):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets, survey_id in data_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            # Obtenir les sorties du modèle\n",
    "            out = model(data)\n",
    "            \n",
    "            # Appliquer la fonction softmax pour obtenir des probabilités\n",
    "            m = nn.Softmax(dim=2)\n",
    "            probs = m(out)\n",
    "            \n",
    "            topk_preds = torch.topk(probs[:, :, 4], 25, dim=1).indices\n",
    "            topk_labels = torch.zeros_like(probs[:, :, 4])\n",
    "            topk_labels.scatter_(1, topk_preds, 1)\n",
    "\n",
    "            all_preds.append(topk_labels.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Convertir les listes en tableaux numpy et les aplatir\n",
    "    all_preds = np.vstack(all_preds).reshape(-1, num_species)\n",
    "    all_targets = np.vstack(all_targets).reshape(-1, num_species)\n",
    "\n",
    "    # Calculer le F1 score\n",
    "    f1 = f1_score(all_targets, all_preds, average='micro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Évaluer le modèle\n",
    "f1 = evaluate_model(model, test_loader, device, num_species)\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests Kaggle\n",
    "class TestDataset(TrainDataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC24-PA-{self.subset}-landsat_time_series_{survey_id}_cube.pt\")))\n",
    "\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, survey_id\n",
    "    \n",
    "# Load Test metadata\n",
    "test_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-test-landsat_time_series\"\n",
    "test_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-metadata-test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path, delimiter =';')\n",
    "test_dataset = TestDataset(test_data_path, test_metadata, subset=\"test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d60be2cc55f4cec94f0157763888b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9775, device='cuda:0')\n",
      "tensor(0.9839, device='cuda:0')\n",
      "tensor(0.9844, device='cuda:0')\n",
      "tensor(0.9781, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0')\n",
      "tensor(0.9764, device='cuda:0')\n",
      "tensor(0.9829, device='cuda:0')\n",
      "tensor(0.9771, device='cuda:0')\n",
      "tensor(0.9778, device='cuda:0')\n",
      "tensor(0.9847, device='cuda:0')\n",
      "tensor(0.9843, device='cuda:0')\n",
      "tensor(0.9828, device='cuda:0')\n",
      "tensor(0.9836, device='cuda:0')\n",
      "tensor(0.9795, device='cuda:0')\n",
      "tensor(0.9842, device='cuda:0')\n",
      "tensor(0.9835, device='cuda:0')\n",
      "tensor(0.9820, device='cuda:0')\n",
      "tensor(0.9770, device='cuda:0')\n",
      "tensor(0.9461, device='cuda:0')\n",
      "tensor(0.9768, device='cuda:0')\n",
      "tensor(0.9795, device='cuda:0')\n",
      "tensor(0.9796, device='cuda:0')\n",
      "tensor(0.9770, device='cuda:0')\n",
      "tensor(0.9847, device='cuda:0')\n",
      "tensor(0.9771, device='cuda:0')\n",
      "tensor(0.9783, device='cuda:0')\n",
      "tensor(0.9813, device='cuda:0')\n",
      "tensor(0.9419, device='cuda:0')\n",
      "tensor(0.9844, device='cuda:0')\n",
      "tensor(0.9786, device='cuda:0')\n",
      "tensor(0.9784, device='cuda:0')\n",
      "tensor(0.9835, device='cuda:0')\n",
      "tensor(0.9774, device='cuda:0')\n",
      "tensor(0.9838, device='cuda:0')\n",
      "tensor(0.9796, device='cuda:0')\n",
      "tensor(0.9744, device='cuda:0')\n",
      "tensor(0.9784, device='cuda:0')\n",
      "tensor(0.9847, device='cuda:0')\n",
      "tensor(0.9775, device='cuda:0')\n",
      "tensor(0.9843, device='cuda:0')\n",
      "tensor(0.9833, device='cuda:0')\n",
      "tensor(0.9803, device='cuda:0')\n",
      "tensor(0.9822, device='cuda:0')\n",
      "tensor(0.9771, device='cuda:0')\n",
      "tensor(0.9770, device='cuda:0')\n",
      "tensor(0.9846, device='cuda:0')\n",
      "tensor(0.9781, device='cuda:0')\n",
      "tensor(0.9778, device='cuda:0')\n",
      "tensor(0.9775, device='cuda:0')\n",
      "tensor(0.9844, device='cuda:0')\n",
      "tensor(0.9834, device='cuda:0')\n",
      "tensor(0.9786, device='cuda:0')\n",
      "tensor(0.9788, device='cuda:0')\n",
      "tensor(0.9771, device='cuda:0')\n",
      "tensor(0.9795, device='cuda:0')\n",
      "tensor(0.9754, device='cuda:0')\n",
      "tensor(0.9771, device='cuda:0')\n",
      "tensor(0.9780, device='cuda:0')\n",
      "tensor(0.9819, device='cuda:0')\n",
      "tensor(0.9848, device='cuda:0')\n",
      "tensor(0.9643, device='cuda:0')\n",
      "tensor(0.9819, device='cuda:0')\n",
      "tensor(0.9781, device='cuda:0')\n",
      "tensor(0.9844, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0')\n",
      "tensor(0.9835, device='cuda:0')\n",
      "tensor(0.9839, device='cuda:0')\n",
      "tensor(0.9828, device='cuda:0')\n",
      "tensor(0.9776, device='cuda:0')\n",
      "tensor(0.9797, device='cuda:0')\n",
      "tensor(0.9780, device='cuda:0')\n",
      "tensor(0.9829, device='cuda:0')\n",
      "tensor(0.9842, device='cuda:0')\n",
      "tensor(0.9776, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    top_k_indices = None\n",
    "    for data, surveyID in tqdm(test_loader, total=len(test_loader)):\n",
    "\n",
    "        data = data.to(device)\n",
    "        preds = model(data)\n",
    "        m = nn.Softmax(dim=2)\n",
    "        probs = m(preds)\n",
    "\n",
    "        predictions = torch.zeros(probs[:, :, 0].shape, device=device)\n",
    "\n",
    "        # Mettez à jour predictions avec les valeurs de probs pour la classe 4\n",
    "        predictions = torch.where(probs.argmax(dim=2) == 0, probs[:, :, 0], predictions)\n",
    "        print(predictions[0,111])\n",
    "\n",
    "        # Convertir predictions sur la mémoire de l'hôte pour utiliser np.argsort\n",
    "        predictions_cpu = predictions.cpu().numpy()\n",
    "\n",
    "        # Sélectionner les top-25 valeurs comme prédictions\n",
    "        top_25 = np.argsort(-predictions_cpu, axis=1)[:, :25]\n",
    "        if top_k_indices is None:\n",
    "            top_k_indices = top_25\n",
    "        else:\n",
    "            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "        surveys.extend(surveyID.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(\"submission.csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
