{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4814f95e-c906-4318-9acd-d5ff6b3b26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e57bff-fc9f-41d7-8afc-dbaf1970f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=[\"speciesId\"]).reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC24-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            #label_id = self.species_mapping[species_id]  # Get consecutive integer label\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "\n",
    "        # Ensure the sample is in the correct format for the transform\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()  # Convert tensor to numpy array\n",
    "            #print(sample.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label, survey_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480c45ca-a3e7-4bc8-95dd-0f9007b7d74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cols = ['surveyId', 'speciesId']\n",
    "\n",
    "num_classes = 11255\n",
    "#Load Training metadata\n",
    "train_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-train-landsat-time-series\"\n",
    "train_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path, delimiter = ';').iloc[:200000][cols]\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, subset=\"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PA-train-landsat-time-series\"\n",
    "test_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24_PA_metadata_train.csv\"\n",
    "test_metadata = pd.read_csv(train_metadata_path, delimiter = ';').iloc[500000:600001][cols]\n",
    "test_dataset = TrainDataset(test_data_path, test_metadata, subset=\"train\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "sample, label, surveyId = train_dataset[0]\n",
    "\n",
    "print(sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90ce9ef-beb8-4d46-9878-3cbf5b60d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "\n",
    "        self.norm_input = nn.LayerNorm([6,4,21])\n",
    "        self.resnet18 = models.resnet18(weights=None)\n",
    "        # We have to modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.resnet18.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet18.maxpool = nn.Identity()\n",
    "        self.ln = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(1000, 2056)\n",
    "        self.fc2 = nn.Linear(2056, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_input(x)\n",
    "        x = self.resnet18(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f612e34-c557-493c-9a83-a8109b81ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "num_classes = 11255 # Number of all unique classes within the PO and PA data.\n",
    "model = ModifiedResNet18(num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a771b7d7-9d5c-4f5c-9887-5eb025afdb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dakbarin/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20\n",
    "positive_weigh_factor = 1.0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b5345c-e8ac-427a-a2c2-eeb19dc09d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs started.\n",
      "Epoch 1/20, Batch 0/189, Loss: 0.7075073719024658\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 1, 'verbose': True, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.0001992114701314478]}\n",
      "Epoch 2/20, Batch 0/189, Loss: 0.006964107975363731\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 2, 'verbose': True, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.0001968583161128631]}\n",
      "Epoch 3/20, Batch 0/189, Loss: 0.006491994485259056\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 3, 'verbose': True, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [0.00019297764858882514]}\n",
      "Epoch 4/20, Batch 0/189, Loss: 0.005732167977839708\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 4, 'verbose': True, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [0.00018763066800438636]}\n",
      "Epoch 5/20, Batch 0/189, Loss: 0.005121965426951647\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 5, 'verbose': True, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [0.00018090169943749476]}\n",
      "Epoch 6/20, Batch 0/189, Loss: 0.00434014480561018\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 6, 'verbose': True, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [0.00017289686274214118]}\n",
      "Epoch 7/20, Batch 0/189, Loss: 0.0048327092081308365\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 7, 'verbose': True, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [0.000163742398974869]}\n",
      "Epoch 8/20, Batch 0/189, Loss: 0.003895278787240386\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 8, 'verbose': True, '_step_count': 9, '_get_lr_called_within_step': False, '_last_lr': [0.00015358267949789966]}\n",
      "Epoch 9/20, Batch 0/189, Loss: 0.004508699756115675\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 9, 'verbose': True, '_step_count': 10, '_get_lr_called_within_step': False, '_last_lr': [0.00014257792915650726]}\n",
      "Epoch 10/20, Batch 0/189, Loss: 0.0040482403710484505\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 10, 'verbose': True, '_step_count': 11, '_get_lr_called_within_step': False, '_last_lr': [0.00013090169943749474]}\n",
      "Epoch 11/20, Batch 0/189, Loss: 0.003801130224019289\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 11, 'verbose': True, '_step_count': 12, '_get_lr_called_within_step': False, '_last_lr': [0.00011873813145857248]}\n",
      "Epoch 12/20, Batch 0/189, Loss: 0.003938526846468449\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 12, 'verbose': True, '_step_count': 13, '_get_lr_called_within_step': False, '_last_lr': [0.00010627905195293135]}\n",
      "Epoch 13/20, Batch 0/189, Loss: 0.003955530468374491\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 13, 'verbose': True, '_step_count': 14, '_get_lr_called_within_step': False, '_last_lr': [9.372094804706867e-05]}\n",
      "Epoch 14/20, Batch 0/189, Loss: 0.00368065875954926\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 14, 'verbose': True, '_step_count': 15, '_get_lr_called_within_step': False, '_last_lr': [8.126186854142755e-05]}\n",
      "Epoch 15/20, Batch 0/189, Loss: 0.003209170186892152\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 15, 'verbose': True, '_step_count': 16, '_get_lr_called_within_step': False, '_last_lr': [6.90983005625053e-05]}\n",
      "Epoch 16/20, Batch 0/189, Loss: 0.003371210303157568\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 16, 'verbose': True, '_step_count': 17, '_get_lr_called_within_step': False, '_last_lr': [5.742207084349274e-05]}\n",
      "Epoch 17/20, Batch 0/189, Loss: 0.003436504164710641\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 17, 'verbose': True, '_step_count': 18, '_get_lr_called_within_step': False, '_last_lr': [4.6417320502100316e-05]}\n",
      "Epoch 18/20, Batch 0/189, Loss: 0.003277987241744995\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 18, 'verbose': True, '_step_count': 19, '_get_lr_called_within_step': False, '_last_lr': [3.6257601025131026e-05]}\n",
      "Epoch 19/20, Batch 0/189, Loss: 0.0031558233313262463\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 19, 'verbose': True, '_step_count': 20, '_get_lr_called_within_step': False, '_last_lr': [2.7103137257858868e-05]}\n",
      "Epoch 20/20, Batch 0/189, Loss: 0.0029393157456070185\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.0002], 'last_epoch': 20, 'verbose': True, '_step_count': 21, '_get_lr_called_within_step': False, '_last_lr': [1.909830056250527e-05]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets, _) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 278 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Scheduler:\",scheduler.state_dict())\n",
    "\n",
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"resnet18-with-bioclimatic-cubes1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d89553c-1a04-432d-bd14-c40dd708cb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score Micro: 0.2493468773626571\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "model = ModifiedResNet18(num_classes).to(device)\n",
    "model.load_state_dict(torch.load('/home/dakbarin/data/models/resnet18-with-bioclimatic-cubes1.pth'))\n",
    "model.to(device)\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels, _ = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Transférer les données sur le même dispositif\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs)  # Utiliser la fonction sigmoid pour obtenir des probabilités\n",
    "            \n",
    "            # Convertir les prédictions et les labels en valeurs binaires\n",
    "            preds_binary = (preds > 0.5).int()\n",
    "            \n",
    "            all_preds.append(preds_binary.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "# Tester le modèle\n",
    "preds, labels = test_model(model, test_loader, device)\n",
    "\n",
    "preds = preds.astype(int)\n",
    "labels = labels.astype(int)\n",
    "\n",
    "# Calculer le F1 score micro\n",
    "f1_micro = f1_score(labels, preds, average='micro')\n",
    "print(f\"F1 Score Micro: {f1_micro}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
