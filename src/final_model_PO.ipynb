{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b071bf9-bc5f-41e3-8401-d847b991986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c928a2f-2fa8-42b3-808f-4a1acdef2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_species = 11255\n",
    "seed = 32\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset  # Type de sous-ensemble (ex: train, val, test)\n",
    "        self.transform = transform  # Transformation à appliquer aux échantillons\n",
    "        self.data_dir = data_dir  # Répertoire contenant les fichiers de données\n",
    "        self.metadata = metadata\n",
    "        # Supprimer les lignes sans speciesId et réinitialiser les index\n",
    "        self.metadata = self.metadata.dropna(subset=[\"speciesId\"]).reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)  # Convertir speciesId en entier\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=[\"group_number\",\"speciesId\"]).reset_index(drop=True)\n",
    "        \n",
    "        self.species_dict = self.metadata.groupby('group_number')['speciesId'].apply(list).to_dict()\n",
    "        self.surveyId_dict = self.metadata.groupby('group_number')['surveyId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"group_number\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Retourne le nombre de surveyId uniques\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        group_number = self.metadata.loc[idx,\"group_number\"]\n",
    "        final_sample = torch.zeros((6,4,21))\n",
    "        survey_ids = self.surveyId_dict.get(group_number, [])\n",
    "        \n",
    "        # Charger tous les tenseurs en une seule fois et les empiler\n",
    "        samples = [torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC24-PO-{self.subset}-landsat_time_series_{survey_id}_cube.pt\"))) \n",
    "                   for survey_id in survey_ids]\n",
    "\n",
    "        # Si aucun tenseur n'a été chargé, renvoyer un tenseur nul\n",
    "        if len(samples) == 0:\n",
    "            final_sample = torch.zeros((6, 4, 21))\n",
    "        else:\n",
    "            stacked_samples = torch.stack(samples)\n",
    "            final_sample = stacked_samples.mean(dim=0)\n",
    "        \n",
    "        species_ids = self.species_dict.get(group_number, [])  # Obtenir la liste des species IDs pour le group_number\n",
    "        label = torch.zeros(num_species)  \n",
    "        \n",
    "        for species_id in species_ids:\n",
    "            label[species_id] = 1\n",
    "\n",
    "        # Assurer que l'échantillon est dans le bon format pour la transformation\n",
    "        if isinstance(final_sample, torch.Tensor):\n",
    "            final_sample = final_sample.permute(1, 2, 0)  # Changer la forme du tenseur de (C, H, W) à (H, W, C)\n",
    "            final_sample = final_sample.numpy()  \n",
    "\n",
    "        if self.transform:\n",
    "            final_sample = self.transform(final_sample)\n",
    "\n",
    "        return final_sample, label, group_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "157ca792-89c6-47d0-9f4c-cf5f0d15ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(NUM_WORKERS)\n",
    "num_species = 11255\n",
    "#Load Training metadata\n",
    "train_data_path = \"/home/dakbarin/data/data/GEOLIFECLEF/GLC24-PO-train-landsat_time_series\"\n",
    "train_metadata_path = \"/home/dakbarin/data/data/GEOLIFECLEF/PO_grouped.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, subset=\"train\", transform=transform)\n",
    "training, validation = random_split(train_dataset,\n",
    "                                    [int(len(train_dataset)*0.85), len(train_dataset)-int(len(train_dataset)*0.85)],\n",
    "                                    generator=torch.Generator().manual_seed(seed))\n",
    "train_loader = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ac4f3a-f0f1-479c-a3f1-4d2785d5aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52873\n",
      "torch.Size([6, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "final_sample, label, group_number = train_dataset[1000]\n",
    "print(len(train_dataset))\n",
    "print(final_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dde1999-ffae-44d0-9a74-4fd236121665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "\n",
    "        self.norm_input = nn.LayerNorm([6,4,21])\n",
    "        self.resnet18 = models.resnet18(weights=None)\n",
    "        # We have to modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.resnet18.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet18.maxpool = nn.Identity()\n",
    "        self.ln = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(1000, 2056)\n",
    "        self.fc2 = nn.Linear(2056, num_species)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_input(x)\n",
    "        x = self.resnet18(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b43a2184-e8ee-4181-9faa-41e99ff6c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "#model = ModifiedResNet18(num_species).to(device)\n",
    "model = ModifiedResNet18(num_species).to(device)\n",
    "state_dict = torch.load(\"/home/dakbarin/data/models/resnet18_with_bioclimatic_cubes_epoch_20.pth\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56196caf-2d5e-4ea8-ad67-a1e38f43a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 11\n",
    "positive_weigh_factor = 1.0\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf61431-0de6-41b4-b418-9bacb959ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 11 epochs started.\n",
      "Epoch 1/11 | Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39366c4c27d249e7ab45afcb149671be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38372/3920757025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "# Structure pour stocker les pertes\n",
    "losses = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": []\n",
    "}\n",
    "    \n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Training...\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, targets, _ in tqdm(train_loader, desc=f\"Training\", leave=False):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "\n",
    "        loss = criterion(out, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Validation...\")\n",
    "        for data, targets, _ in tqdm(val_loader, desc=f\"Validation)\", leave=False):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            out = model(data)\n",
    "\n",
    "            loss = criterion(out, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(test_loader)}\")\n",
    "\n",
    "    # Enregistrer les pertes dans la structure\n",
    "    losses[\"epoch\"].append(epoch + 1)\n",
    "    losses[\"train_loss\"].append(running_loss / len(train_loader))\n",
    "    losses[\"val_loss\"].append(val_loss / len(test_loader))\n",
    "\n",
    "    # Sauvegarder les pertes dans un fichier CSV\n",
    "    df = pd.DataFrame(losses)\n",
    "    df.to_csv(\"training_losses.csv\", index=False)\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(model.state_dict(), f\"resnet18_with_bioclimatic_cubes_epoch_{epoch}_fine_tuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
